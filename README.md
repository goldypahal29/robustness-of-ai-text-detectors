robustness-of-ai-text-detectors

This repository contains the source code, datasets, and experimental configurations used in the research paper titled "Evaluating Robustness of Neural Text Detectors in Generative AI Detection". The study aims to systematically analyze and compare the effectiveness and robustness of various state-of-the-art AI-generated text detection models under different conditions.

The research evaluates a diverse set of detection approaches, including classical machine learning models such as Logistic Regression and Random Forest, as well as popular commercial and academic detectors like Turnitin, GPTZero, and others. The evaluation focuses on multiple performance metrics such as accuracy, precision, recall, and robustness against adversarial or obfuscated AI-generated texts.

By providing a comprehensive benchmark and analysis, this work contributes valuable insights into the strengths and limitations of current neural text detectors, helping researchers and practitioners understand which models perform best in real-world scenarios where distinguishing human-written from AI-generated content is increasingly challenging.
